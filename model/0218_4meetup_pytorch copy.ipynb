{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "seed_everything(37) # Seed 고정\n",
    "\n",
    "train_df = pd.read_csv('./train.csv')\n",
    "test_df = pd.read_csv('./test.csv')\n",
    "\n",
    "#train_x = train_df.drop(columns=['PRODUCT_ID', 'Y_Class', 'TIMESTAMP'])\n",
    "train_x = train_df.drop(columns=['PRODUCT_ID', 'Y_Quality', 'TIMESTAMP'])\n",
    "train_y = train_df['Y_Class']\n",
    "\n",
    "test_x = test_df.drop(columns=['PRODUCT_ID', 'TIMESTAMP'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "qual_col = ['LINE','PRODUCT_CODE']\n",
    "\n",
    "for i in qual_col:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_x[i])\n",
    "    train_x[i] = le.transform(train_x[i])\n",
    "\n",
    "    for label in np.unique(test_x[i]):\n",
    "        if label not in le.classes_:\n",
    "            le.classes_ = np.append(le.classes_, label)\n",
    "    \n",
    "    test_x[i] = le.transform(test_x[i])\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x_1 shape : (249, 2877) \n",
      "train_x_2 shape : (6, 2877) \n",
      "train_x_3 shape : (343, 2877)\n",
      "train\n",
      "(59, 2877)\n",
      "(70, 2877)\n",
      "(78, 2877)\n",
      "(42, 2877)\n",
      "(3, 2877)\n",
      "(172, 2877)\n",
      "(3, 2877)\n",
      "(171, 2877)\n",
      "\n",
      "\n",
      "test\n",
      "(14, 2877)\n",
      "(14, 2877)\n",
      "(13, 2877)\n",
      "(26, 2877)\n",
      "(3, 2877)\n",
      "(108, 2877)\n",
      "(1, 2877)\n",
      "(131, 2877)\n"
     ]
    }
   ],
   "source": [
    "## PRODUCT_CODE\n",
    "\n",
    "# train\n",
    "train_x_1 = train_x[train_x['PRODUCT_CODE'] == 0].drop('Y_Class', axis=1)\n",
    "train_x_2 = train_x[train_x['PRODUCT_CODE'] == 1].drop('Y_Class', axis=1)\n",
    "train_x_3 = train_x[train_x['PRODUCT_CODE'] == 2].drop('Y_Class', axis=1)\n",
    "\n",
    "train_y_1 = train_x['Y_Class'][train_x['PRODUCT_CODE'] == 0]\n",
    "train_y_2 = train_x['Y_Class'][train_x['PRODUCT_CODE'] == 1]\n",
    "train_y_3 = train_x['Y_Class'][train_x['PRODUCT_CODE'] == 2]\n",
    "\n",
    "print('train_x_1 shape :', train_x_1.shape,\n",
    "      '\\ntrain_x_2 shape :', train_x_2.shape,\n",
    "      '\\ntrain_x_3 shape :', train_x_3.shape)\n",
    "\n",
    "# test\n",
    "test_x_1 = test_x[test_x['PRODUCT_CODE'] == 0]\n",
    "test_x_2 = test_x[test_x['PRODUCT_CODE'] == 1]\n",
    "test_x_3 = test_x[test_x['PRODUCT_CODE'] == 2]\n",
    "\n",
    "\n",
    "\n",
    "# LINE\n",
    "\n",
    "## TRAIN\n",
    "\n",
    "# line 1\n",
    "test_x_1_1 = test_x_1[test_x_1['LINE'] == 0]\n",
    "\n",
    "# line 2\n",
    "test_x_1_2 = test_x_1[test_x_1['LINE'] == 1]\n",
    "\n",
    "# line 3\n",
    "test_x_1_3 = test_x_1[test_x_1['LINE'] == 2]\n",
    "\n",
    "# line 4\n",
    "test_x_1_4 = test_x_1[test_x_1['LINE'] == 3]\n",
    "\n",
    "# line 5\n",
    "test_x_2_5 = test_x_2[test_x_2['LINE'] == 4]\n",
    "test_x_3_5 = test_x_3[test_x_3['LINE'] == 4]\n",
    "\n",
    "# line 6\n",
    "test_x_2_6 = test_x_2[test_x_2['LINE'] == 5]\n",
    "test_x_3_6 = test_x_3[test_x_3['LINE'] == 5]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## TRAIN\n",
    "\n",
    "# line 1\n",
    "train_x_1_1 = train_x_1[train_x_1['LINE'] == 0]\n",
    "\n",
    "# line 2\n",
    "train_x_1_2 = train_x_1[train_x_1['LINE'] == 1]\n",
    "\n",
    "# line 3\n",
    "train_x_1_3 = train_x_1[train_x_1['LINE'] == 2]\n",
    "\n",
    "# line 4\n",
    "train_x_1_4 = train_x_1[train_x_1['LINE'] == 3]\n",
    "\n",
    "# line 5\n",
    "train_x_2_5 = train_x_2[train_x_2['LINE'] == 4]\n",
    "train_x_3_5 = train_x_3[train_x_3['LINE'] == 4]\n",
    "\n",
    "# line 6\n",
    "train_x_2_6 = train_x_2[train_x_2['LINE'] == 5]\n",
    "train_x_3_6 = train_x_3[train_x_3['LINE'] == 5]\n",
    "\n",
    "\n",
    "train_set = [train_x_1_1, train_x_1_2, train_x_1_3, train_x_1_4, train_x_2_5, train_x_3_5, train_x_2_6, train_x_3_6]\n",
    "test_set = [test_x_1_1, test_x_1_2, test_x_1_3, test_x_1_4, test_x_2_5, test_x_3_5, test_x_2_6, test_x_3_6]\n",
    "\n",
    "print('train')\n",
    "for set in train_set:\n",
    "    print(set.shape)\n",
    "\n",
    "print('\\n\\ntest')\n",
    "for set in test_set:\n",
    "    print(set.shape)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### null - mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "train_set_mean = train_set\n",
    "test_set_mean = test_set\n",
    "\n",
    "for set in train_set_mean:\n",
    "    cols = set.columns\n",
    "    for col in cols:\n",
    "        set[col] = set[col].fillna(set[col].mean())\n",
    "\n",
    "for set in test_set_mean:\n",
    "    cols = set.columns\n",
    "    for col in cols:\n",
    "        set[col] = set[col].fillna(set[col].mean())\n",
    "\n",
    "train_x = pd.concat(train_set_mean, axis=0).sort_index()\n",
    "test_x = pd.concat(test_set_mean, axis=0).sort_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파생변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "new features \n",
    "\"\"\"\n",
    "\n",
    "train_x['LINE_PRODUCT_CODE'] = train_x[['LINE','PRODUCT_CODE']].apply(lambda x: '-'.join(x.astype(str)),axis=1)\n",
    "test_x['LINE_PRODUCT_CODE'] = test_x[['LINE','PRODUCT_CODE']].apply(lambda x: '-'.join(x.astype(str)),axis=1)\n",
    "\n",
    "train_x.drop(['LINE','PRODUCT_CODE'], axis=1, inplace=True)\n",
    "test_x.drop(['LINE','PRODUCT_CODE'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "qual_col = ['LINE_PRODUCT_CODE']\n",
    "\n",
    "for i in qual_col:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_x[i])\n",
    "    train_x[i] = le.transform(train_x[i])\n",
    "\n",
    "    for label in np.unique(test_x[i]):\n",
    "        if label not in le.classes_:\n",
    "            le.classes_ = np.append(le.classes_, label)\n",
    "    \n",
    "    test_x[i] = le.transform(test_x[i])\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "train_cols = train_x.columns.drop('LINE_PRODUCT_CODE')\n",
    "test_cols = test_x.columns.drop('LINE_PRODUCT_CODE')\n",
    "\n",
    "for col in train_cols:\n",
    "    train_x[col] = zscore(train_x[col])\n",
    "\n",
    "for col in test_cols:\n",
    "    test_x[col] = zscore(test_x[col])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.fillna(0)\n",
    "test_x = test_x.fillna(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(598, 2876)\n",
      "(598,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_t, x_v, y_t, y_v = train_test_split(train_x, train_y, test_size=0.2, random_state=42)\n",
    "\n",
    "x_torch = train_x\n",
    "y_torch = train_y\n",
    "\n",
    "print(x_torch.shape)\n",
    "print(y_torch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(598, 2876)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_columns = x_torch.columns\n",
    "x = x_torch[x_columns].values\n",
    "y = np.array(y_torch)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y_torch)\n",
    "\n",
    "products = le.classes_\n",
    "products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: using Google CoLab\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import io\n",
    "import copy\n",
    "\n",
    "try:\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# Early stopping (see Module 3.4)\n",
    "class EarlyStopping():\n",
    "  def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n",
    "    self.patience = patience\n",
    "    self.min_delta = min_delta\n",
    "    self.restore_best_weights = restore_best_weights\n",
    "    self.best_model = None\n",
    "    self.best_loss = None\n",
    "    self.counter = 0\n",
    "    self.status = \"\"\n",
    "    \n",
    "  def __call__(self, model, val_loss):\n",
    "    if self.best_loss == None:\n",
    "      self.best_loss = val_loss\n",
    "      self.best_model = copy.deepcopy(model)\n",
    "    elif self.best_loss - val_loss > self.min_delta:\n",
    "      self.best_loss = val_loss\n",
    "      self.counter = 0\n",
    "      self.best_model.load_state_dict(model.state_dict())\n",
    "    elif self.best_loss - val_loss < self.min_delta:\n",
    "      self.counter += 1\n",
    "      if self.counter >= self.patience:\n",
    "        self.status = f\"Stopped on {self.counter}\"\n",
    "        if self.restore_best_weights:\n",
    "          model.load_state_dict(self.best_model.state_dict())\n",
    "        return True\n",
    "    self.status = f\"{self.counter}/{self.patience}\"\n",
    "    return False\n",
    "\n",
    "# Make use of a GPU or MPS (Apple) if one is available. (see Module 3.2)\n",
    "device = \"mps\" if getattr(torch,'has_mps',False) \\\n",
    "    else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from sklearn import preprocessing\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "# Define the PyTorch Neural Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_count, out_count):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_count, 50)\n",
    "        self.fc2 = nn.Linear(50, 25)\n",
    "        self.fc3 = nn.Linear(25, out_count)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.softmax(self.fc3(x))\n",
    "\n",
    "# Split into validation and training sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Numpy to Torch Tensor\n",
    "x_train = torch.Tensor(x_train).float()\n",
    "y_train = torch.Tensor(y_train).long()\n",
    "\n",
    "x_test = torch.Tensor(x_test).float().to(device)\n",
    "y_test = torch.Tensor(y_test).long().to(device)\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "dataset_train = TensorDataset(x_train, y_train)\n",
    "dataloader_train = DataLoader(dataset_train,\\\n",
    "  batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "dataset_test = TensorDataset(x_test, y_test)\n",
    "dataloader_test = DataLoader(dataset_test,\\\n",
    "  batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = Net(x.shape[1],len(products)).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()# cross entropy loss\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters()) # , lr=0.01\n",
    "es = EarlyStopping()\n",
    "\n",
    "epoch = 0\n",
    "done = False\n",
    "while epoch<1000 and not done:\n",
    "  epoch += 1\n",
    "  steps = list(enumerate(dataloader_train))\n",
    "  pbar = tqdm.tqdm(steps)\n",
    "  model.train()\n",
    "  for i, (x_batch, y_batch) in pbar:\n",
    "    y_batch_pred = model(x_batch.to(device))\n",
    "    loss = loss_fn(y_batch_pred, y_batch.to(device))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss, current = loss.item(), (i + 1)* len(x_batch)\n",
    "    if i == len(steps)-1:\n",
    "      model.eval()\n",
    "      pred = model(x_test)\n",
    "      vloss = loss_fn(pred, y_test)\n",
    "      if es(model,vloss): done = True\n",
    "      pbar.set_description(f\"Epoch: {epoch}, tloss: {loss}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "    else:\n",
    "      pbar.set_description(f\"Epoch: {epoch}, tloss {loss:}\")\n",
    "\n",
    "\n",
    "pred = model(x_test)\n",
    "_, predict_classes = torch.max(pred, 1)\n",
    "\n",
    "predict_classes = predict_classes.tolist()\n",
    "y_test = y_test.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.6333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "score = metrics.accuracy_score(y_test, predict_classes)\n",
    "print(\"Accuracy score: {}\".format(score))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.fillna(0)\n",
    "test_x = test_x.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(598, 2876)\n",
      "(598,)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from numpy import genfromtxt\n",
    "\n",
    "train_x.to_csv('train_torch.csv', index_label=False)\n",
    "\n",
    "\n",
    "data = genfromtxt('train_torch.csv', delimiter=',', skip_header=1)[:,1:]\n",
    "input = data\n",
    "target = np.array(train_y)\n",
    "\n",
    "print(input.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([598, 2876])\n",
      "torch.Size([598])\n"
     ]
    }
   ],
   "source": [
    "input = torch.FloatTensor(input)\n",
    "target = torch.LongTensor(target).squeeze()\n",
    "\n",
    "print(input.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(train_y)\n",
    "\n",
    "products = le.classes_\n",
    "products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before pred : tensor([[0.3372, 0.3335, 0.3293],\n",
      "        [0.3379, 0.3330, 0.3291],\n",
      "        [0.3372, 0.3335, 0.3293],\n",
      "        ...,\n",
      "        [0.3372, 0.3335, 0.3293],\n",
      "        [0.3386, 0.3325, 0.3288],\n",
      "        [0.3400, 0.3316, 0.3284]], grad_fn=<SoftmaxBackward0>)\n",
      "before loss : tensor(1.0994, grad_fn=<NllLossBackward0>)\n",
      "after pred : tensor([[3.5530e-02, 9.0778e-01, 5.6694e-02],\n",
      "        [1.4328e-02, 9.5916e-01, 2.6510e-02],\n",
      "        [3.5530e-02, 9.0778e-01, 5.6694e-02],\n",
      "        ...,\n",
      "        [3.5530e-02, 9.0778e-01, 5.6694e-02],\n",
      "        [5.6006e-03, 9.8238e-01, 1.2016e-02],\n",
      "        [8.2772e-04, 9.9678e-01, 2.3876e-03]], grad_fn=<SoftmaxBackward0>)\n",
      "after loss : tensor(0.8729, grad_fn=<NllLossBackward0>)\n",
      "[Parameter containing:\n",
      "tensor([[ 1.5704e-02, -1.2264e-02,  1.5727e-02,  ...,  1.6666e-02,\n",
      "         -4.9007e-04, -3.7153e-01],\n",
      "        [-6.1633e-03, -1.5590e-03,  9.1266e-03,  ..., -1.5089e-02,\n",
      "         -1.2696e-02,  5.9172e-01],\n",
      "        [ 5.2213e-03,  1.4187e-02,  1.2306e-02,  ...,  1.4734e-02,\n",
      "         -7.2955e-03, -2.2351e-01]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4942,  0.8199, -0.3230], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(input.shape[1],len(products)),\n",
    "                      nn.Softmax(dim=1))\n",
    "\n",
    "pred = model(input)\n",
    "print('before pred :', pred)\n",
    "loss = F.cross_entropy(pred, target)\n",
    "print('before loss :', loss)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "epoches = 1000\n",
    "\n",
    "for epoch in range(epoches + 1):\n",
    "    pred = model(input)\n",
    "\n",
    "    loss = F.cross_entropy(pred, target=target)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "pred = model(input)\n",
    "print('after pred :', pred)\n",
    "loss = F.cross_entropy(pred, target)\n",
    "print('after loss :', loss)\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 4)\n",
      "(30, 1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from numpy import genfromtxt\n",
    "\n",
    "data = genfromtxt('./IRIS_tiny.csv', delimiter=',', skip_header=1)\n",
    "input = data[:, 0:4]\n",
    "target = data[:, -1:]\n",
    "\n",
    "print(input.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 4])\n",
      "torch.Size([30])\n"
     ]
    }
   ],
   "source": [
    "input = torch.FloatTensor(input)\n",
    "target = torch.LongTensor(target).squeeze()\n",
    "\n",
    "print(input.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before pred : tensor([[0.1395, 0.3082, 0.5523],\n",
      "        [0.1371, 0.3188, 0.5441],\n",
      "        [0.1474, 0.3176, 0.5350],\n",
      "        [0.1521, 0.3189, 0.5289],\n",
      "        [0.1453, 0.3073, 0.5473],\n",
      "        [0.1315, 0.2775, 0.5910],\n",
      "        [0.1527, 0.3044, 0.5429],\n",
      "        [0.1426, 0.3102, 0.5471],\n",
      "        [0.1545, 0.3248, 0.5207],\n",
      "        [0.1445, 0.3265, 0.5290],\n",
      "        [0.0688, 0.1698, 0.7615],\n",
      "        [0.0779, 0.1698, 0.7523],\n",
      "        [0.0684, 0.1640, 0.7676],\n",
      "        [0.0926, 0.2068, 0.7005],\n",
      "        [0.0720, 0.1721, 0.7559],\n",
      "        [0.0985, 0.1974, 0.7041],\n",
      "        [0.0798, 0.1623, 0.7580],\n",
      "        [0.1167, 0.2413, 0.6419],\n",
      "        [0.0769, 0.1854, 0.7377],\n",
      "        [0.1022, 0.1979, 0.6999],\n",
      "        [0.0613, 0.1051, 0.8336],\n",
      "        [0.0784, 0.1511, 0.7705],\n",
      "        [0.0539, 0.1215, 0.8246],\n",
      "        [0.0758, 0.1499, 0.7743],\n",
      "        [0.0618, 0.1217, 0.8164],\n",
      "        [0.0493, 0.1157, 0.8350],\n",
      "        [0.1025, 0.1781, 0.7194],\n",
      "        [0.0594, 0.1380, 0.8026],\n",
      "        [0.0644, 0.1484, 0.7872],\n",
      "        [0.0483, 0.0958, 0.8559]], grad_fn=<SoftmaxBackward0>)\n",
      "before loss : tensor(1.0924, grad_fn=<NllLossBackward0>)\n",
      "after pred : tensor([[0.9053, 0.0853, 0.0095],\n",
      "        [0.8464, 0.1358, 0.0179],\n",
      "        [0.8801, 0.1061, 0.0139],\n",
      "        [0.8441, 0.1354, 0.0204],\n",
      "        [0.9133, 0.0779, 0.0088],\n",
      "        [0.9113, 0.0788, 0.0099],\n",
      "        [0.8860, 0.0992, 0.0148],\n",
      "        [0.8858, 0.1017, 0.0125],\n",
      "        [0.8248, 0.1509, 0.0244],\n",
      "        [0.8546, 0.1294, 0.0160],\n",
      "        [0.1413, 0.4983, 0.3603],\n",
      "        [0.1417, 0.4510, 0.4073],\n",
      "        [0.0965, 0.4723, 0.4312],\n",
      "        [0.0897, 0.4536, 0.4567],\n",
      "        [0.0877, 0.4602, 0.4521],\n",
      "        [0.0952, 0.4395, 0.4653],\n",
      "        [0.1174, 0.4209, 0.4618],\n",
      "        [0.2086, 0.4671, 0.3243],\n",
      "        [0.1150, 0.4978, 0.3872],\n",
      "        [0.1290, 0.4169, 0.4541],\n",
      "        [0.0113, 0.1859, 0.8028],\n",
      "        [0.0265, 0.2945, 0.6790],\n",
      "        [0.0179, 0.2989, 0.6831],\n",
      "        [0.0240, 0.3145, 0.6615],\n",
      "        [0.0150, 0.2473, 0.7377],\n",
      "        [0.0092, 0.2763, 0.7145],\n",
      "        [0.0392, 0.3051, 0.6557],\n",
      "        [0.0144, 0.3309, 0.6547],\n",
      "        [0.0135, 0.3141, 0.6723],\n",
      "        [0.0193, 0.2419, 0.7387]], grad_fn=<SoftmaxBackward0>)\n",
      "after loss : tensor(0.7981, grad_fn=<NllLossBackward0>)\n",
      "[Parameter containing:\n",
      "tensor([[-0.1282,  0.9394, -0.3525, -0.5688],\n",
      "        [-0.1824, -0.1036,  0.5301, -0.1519],\n",
      "        [-0.5524, -0.2961,  0.9710,  0.7338]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.2774, -0.0321, -0.4627], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "input = torch.FloatTensor(input)\n",
    "target = torch.LongTensor(target).squeeze()\n",
    "\n",
    "model = nn.Sequential(nn.Linear(4,3),\n",
    "                      nn.Softmax(dim=1))\n",
    "\n",
    "pred = model(input)\n",
    "print('before pred :', pred)\n",
    "loss = F.cross_entropy(pred, target)\n",
    "print('before loss :', loss)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "epoches = 100\n",
    "\n",
    "for epoch in range(epoches + 1):\n",
    "    pred = model(input)\n",
    "\n",
    "    loss = F.cross_entropy(pred, target=target)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "pred = model(input)\n",
    "print('after pred :', pred)\n",
    "loss = F.cross_entropy(pred, target)\n",
    "print('after loss :', loss)\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(478, 2877)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_t, x_v, y_t, y_v = train_test_split(train_x, train_y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "dataframe = x_t\n",
    "dataframe['target'] = y_t\n",
    "\n",
    "print(dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,  loss:0.1875\n",
      "epoch:1,  loss:0.1875\n",
      "epoch:2,  loss:0.1875\n",
      "epoch:3,  loss:0.1875\n",
      "epoch:4,  loss:0.1875\n",
      "epoch:5,  loss:0.1875\n",
      "epoch:6,  loss:0.1875\n",
      "epoch:7,  loss:0.1875\n",
      "epoch:8,  loss:0.1875\n",
      "epoch:9,  loss:0.1875\n",
      "epoch:10,  loss:0.1875\n",
      "epoch:11,  loss:0.1875\n",
      "epoch:12,  loss:0.1875\n",
      "epoch:13,  loss:0.1875\n",
      "epoch:14,  loss:0.1875\n",
      "epoch:15,  loss:0.1875\n",
      "epoch:16,  loss:0.1875\n",
      "epoch:17,  loss:0.1875\n",
      "epoch:18,  loss:0.1875\n",
      "epoch:19,  loss:0.1875\n",
      "epoch:20,  loss:0.1875\n",
      "epoch:21,  loss:0.1875\n",
      "epoch:22,  loss:0.1875\n",
      "epoch:23,  loss:0.1875\n",
      "epoch:24,  loss:0.1875\n",
      "epoch:25,  loss:0.1875\n",
      "epoch:26,  loss:0.1875\n",
      "epoch:27,  loss:0.1875\n",
      "epoch:28,  loss:0.1875\n",
      "epoch:29,  loss:0.1875\n",
      "epoch:30,  loss:0.1875\n",
      "epoch:31,  loss:0.1875\n",
      "epoch:32,  loss:0.1875\n",
      "epoch:33,  loss:0.1875\n",
      "epoch:34,  loss:0.1875\n",
      "epoch:35,  loss:0.1875\n",
      "epoch:36,  loss:0.1875\n",
      "epoch:37,  loss:0.1875\n",
      "epoch:38,  loss:0.1875\n",
      "epoch:39,  loss:0.1875\n",
      "epoch:40,  loss:0.1875\n",
      "epoch:41,  loss:0.1875\n",
      "epoch:42,  loss:0.1875\n",
      "epoch:43,  loss:0.1875\n",
      "epoch:44,  loss:0.1875\n",
      "epoch:45,  loss:0.1875\n",
      "epoch:46,  loss:0.1875\n",
      "epoch:47,  loss:0.1875\n",
      "epoch:48,  loss:0.1875\n",
      "epoch:49,  loss:0.1875\n",
      "epoch:50,  loss:0.1875\n",
      "epoch:51,  loss:0.1875\n",
      "epoch:52,  loss:0.1875\n",
      "epoch:53,  loss:0.1875\n",
      "epoch:54,  loss:0.1875\n",
      "epoch:55,  loss:0.1875\n",
      "epoch:56,  loss:0.1875\n",
      "epoch:57,  loss:0.1875\n",
      "epoch:58,  loss:0.1875\n",
      "epoch:59,  loss:0.1875\n",
      "epoch:60,  loss:0.1875\n",
      "epoch:61,  loss:0.1875\n",
      "epoch:62,  loss:0.1875\n",
      "epoch:63,  loss:0.1875\n",
      "epoch:64,  loss:0.1875\n",
      "epoch:65,  loss:0.1875\n",
      "epoch:66,  loss:0.1875\n",
      "epoch:67,  loss:0.1875\n",
      "epoch:68,  loss:0.1875\n",
      "epoch:69,  loss:0.1875\n",
      "epoch:70,  loss:0.1875\n",
      "epoch:71,  loss:0.1875\n",
      "epoch:72,  loss:0.1875\n",
      "epoch:73,  loss:0.1875\n",
      "epoch:74,  loss:0.1875\n",
      "epoch:75,  loss:0.1875\n",
      "epoch:76,  loss:0.1875\n",
      "epoch:77,  loss:0.1875\n",
      "epoch:78,  loss:0.1875\n",
      "epoch:79,  loss:0.1875\n",
      "epoch:80,  loss:0.1875\n",
      "epoch:81,  loss:0.1875\n",
      "epoch:82,  loss:0.1875\n",
      "epoch:83,  loss:0.1875\n",
      "epoch:84,  loss:0.1875\n",
      "epoch:85,  loss:0.1875\n",
      "epoch:86,  loss:0.1875\n",
      "epoch:87,  loss:0.1875\n",
      "epoch:88,  loss:0.1875\n",
      "epoch:89,  loss:0.1875\n",
      "epoch:90,  loss:0.1875\n",
      "epoch:91,  loss:0.1875\n",
      "epoch:92,  loss:0.1875\n",
      "epoch:93,  loss:0.1875\n",
      "epoch:94,  loss:0.1875\n",
      "epoch:95,  loss:0.1875\n",
      "epoch:96,  loss:0.1875\n",
      "epoch:97,  loss:0.1875\n",
      "epoch:98,  loss:0.1875\n",
      "epoch:99,  loss:0.1875\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.adam import Adam\n",
    "\n",
    "# 모델 정의\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(dataframe.shape[1]-1, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100,1)\n",
    ")\n",
    "\n",
    "X = dataframe.iloc[:, :-1].values\n",
    "Y = dataframe.iloc[:, -1].values\n",
    "\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "\n",
    "# 가중치 수정하는 최적화함수 정의\n",
    "optim = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# epoch 반복\n",
    "epoches = 100\n",
    "\n",
    "for epoch in range(epoches):\n",
    "\n",
    "    # 배치 반복\n",
    "    for i in range(len(x) // batch_size):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "\n",
    "        # 파이토치 실수형 텐서로 변환\n",
    "        x = torch.FloatTensor(X[start:end])\n",
    "        y = torch.FloatTensor(Y[start:end])\n",
    "\n",
    "        optim.zero_grad()\n",
    "        preds = model(x)\n",
    "        loss = nn.MSELoss()(preds, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"epoch:{epoch},  loss:{loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x2875 and 2876x100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prediction \u001b[39m=\u001b[39m model(torch\u001b[39m.\u001b[39;49mFloatTensor(X[\u001b[39m0\u001b[39;49m, :\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]))\n\u001b[1;32m      2\u001b[0m real \u001b[39m=\u001b[39m Y[\u001b[39m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m Y[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x2875 and 2876x100)"
     ]
    }
   ],
   "source": [
    "prediction = model(torch.FloatTensor(X[0, :-1]))\n",
    "real = Y[0]\n",
    "\n",
    "Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:49:48] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_4f_b8pp6bp/croot/xgboost-split_1675119661934/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier # 회귀트리\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(train_x, train_y)\n",
    "pred = xgb.predict(test_x)\n",
    "\n",
    "\n",
    "submit_csv = pd.read_csv('./sample_submission.csv')\n",
    "submit_csv['Y_Class'] = pred\n",
    "submit_csv.to_csv('XGBoost_notnull_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRFClassifier\n",
    "\n",
    "xgb = XGBRFClassifier()\n",
    "xgb.fit(train_x, train_y)\n",
    "pred = xgb.predict(test_x)\n",
    "\n",
    "\n",
    "submit_csv = pd.read_csv('./sample_submission.csv')\n",
    "submit_csv['Y_Class'] = pred\n",
    "submit_csv.to_csv('XGBoost_notnull_XGBRFC.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9acc76a459d7db86261bc3c0fc1f5dcdadf57ce90dda3656ef3156477f1c2288"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
