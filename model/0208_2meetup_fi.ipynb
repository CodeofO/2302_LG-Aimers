{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "seed_everything(37) # Seed 고정"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1️⃣ Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('./train.csv')\n",
    "test_df = pd.read_csv('./test.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1)_ drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_df.drop(columns=['PRODUCT_ID', 'Y_Quality', 'TIMESTAMP'])\n",
    "train_y = train_df['Y_Class']\n",
    "test_x = test_df.drop(columns=['PRODUCT_ID', 'TIMESTAMP'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2)_ LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# qualitative to quantitative\n",
    "qual_col = ['LINE', 'PRODUCT_CODE']\n",
    "\n",
    "for i in qual_col:\n",
    "    le = LabelEncoder()\n",
    "    le = le.fit(train_x[i])\n",
    "    train_x[i] = le.transform(train_x[i])\n",
    "    \n",
    "    for label in np.unique(test_x[i]): \n",
    "        if label not in le.classes_: \n",
    "            le.classes_ = np.append(le.classes_, label)\n",
    "    test_x[i] = le.transform(test_x[i]) \n",
    "print('Done.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3)_ Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x['count_null']=train_x.isnull().sum(axis=1)\n",
    "test_x['count_null']=test_x.isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x_1 shape : (249, 2878) \n",
      "train_x_2 shape : (349, 2878)\n",
      "\n",
      "train_y_1 shape : (249,) \n",
      "train_y_2 shape : (349,)\n",
      "\n",
      "test_x_1 shape : (67, 2878) \n",
      "test_x_2 shape : (243, 2878)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "train_x_1 = train_x[train_x['PRODUCT_CODE'] == 0].drop('Y_Class', axis=1)\n",
    "train_x_2 = train_x[train_x['PRODUCT_CODE'] != 0].drop('Y_Class', axis=1)\n",
    "\n",
    "train_y_1 = train_x['Y_Class'][train_x['PRODUCT_CODE'] == 0]\n",
    "train_y_2 = train_x['Y_Class'][train_x['PRODUCT_CODE'] != 0]\n",
    "\n",
    "print('train_x_1 shape :', train_x_1.shape,\n",
    "      '\\ntrain_x_2 shape :', train_x_2.shape)\n",
    "\n",
    "print('\\ntrain_y_1 shape :', train_y_1.shape,\n",
    "      '\\ntrain_y_2 shape :', train_y_2.shape)\n",
    "\n",
    "\n",
    "\n",
    "# test\n",
    "test_x_1 = test_x[test_x['PRODUCT_CODE'] == 0]\n",
    "test_x_2 = test_x[test_x['PRODUCT_CODE'] != 0]\n",
    "\n",
    "print('\\ntest_x_1 shape :', test_x_1.shape,\n",
    "      '\\ntest_x_2 shape :', test_x_2.shape)\n",
    "\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4)_ null 처리"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero\n",
    "train_x_1_zero = train_x_1.fillna(0)\n",
    "train_x_2_zero = train_x_2.fillna(0)\n",
    "\n",
    "# zero\n",
    "test_x_1_zero = test_x_1.fillna(0)\n",
    "test_x_2_zero = test_x_2.fillna(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### drop\n",
    "  \n",
    "test-train 데이터와 동일한 column을 drop 시켜줘야 한다 🤔🤔🤔🤔🤔   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "train_x_isnull = train_x_1.isnull().sum()\n",
    "train_1_drop_col = train_x_isnull[train_x_isnull > 0].index\n",
    "test_x_isnull = test_x_1.isnull().sum()\n",
    "test_1_drop_col = test_x_isnull[test_x_isnull > 0].index\n",
    "\n",
    "\n",
    "train_x_1_drop = train_x_1.drop(test_1_drop_col, axis=1).dropna(axis=1)\n",
    "test_x_1_drop = test_x_1.drop(train_1_drop_col, axis=1).dropna(axis=1)\n",
    "\n",
    "# 2\n",
    "train_x_isnull = train_x_2.isnull().sum()\n",
    "train_2_drop_col = train_x_isnull[train_x_isnull > 0].index\n",
    "test_x_isnull = test_x_2.isnull().sum()\n",
    "test_2_drop_col = test_x_isnull[test_x_isnull > 0].index\n",
    "\n",
    "train_x_2_drop = train_x_2.drop(test_2_drop_col, axis=1).dropna(axis=1)\n",
    "test_x_2_drop = test_x_2.drop(train_2_drop_col, axis=1).dropna(axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shape check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 1\n",
      "(249, 2878)\n",
      "(67, 2878)\n",
      "\n",
      "✅ 2\n",
      "(349, 2878)\n",
      "(243, 2878)\n"
     ]
    }
   ],
   "source": [
    "print('✅ 1')\n",
    "print(train_x_1_zero.shape)\n",
    "print(test_x_1_zero.shape)\n",
    "\n",
    "print('\\n✅ 2')\n",
    "print(train_x_2_zero.shape)\n",
    "print(test_x_2_zero.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 1\n",
      "(249, 221)\n",
      "(67, 221)\n",
      "\n",
      "✅ 2\n",
      "(349, 233)\n",
      "(243, 233)\n"
     ]
    }
   ],
   "source": [
    "print('✅ 1')\n",
    "print(train_x_1_drop.shape)\n",
    "print(test_x_1_drop.shape)\n",
    "\n",
    "print('\\n✅ 2')\n",
    "print(train_x_2_drop.shape)\n",
    "print(test_x_2_drop.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5)_standardscaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_1_d = StandardScaler()\n",
    "scaler_2_z = StandardScaler()\n",
    "scaler_2_d = StandardScaler()\n",
    "scaler_1_z = StandardScaler()\n",
    "\n",
    "\n",
    "# 1\n",
    "### zero\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df = train_x_1_zero\n",
    "scale = scaler.fit_transform(df)\n",
    "train_x_1_zero = pd.DataFrame(data=scale, columns=df.columns, index=df.index)\n",
    "\n",
    "df = test_x_1_zero\n",
    "scale = scaler.transform(df)\n",
    "test_x_1_zero = pd.DataFrame(data=scale, columns=df.columns, index=df.index)\n",
    "\n",
    "\n",
    "### drop\n",
    "df = train_x_1_drop\n",
    "scale = scaler.fit_transform(df)\n",
    "train_x_1_drop = pd.DataFrame(data=scale, columns=df.columns, index=df.index)\n",
    "\n",
    "df = test_x_1_drop\n",
    "scale = scaler.transform(df)\n",
    "test_x_1_drop = pd.DataFrame(data=scale, columns=df.columns, index=df.index)\n",
    "\n",
    "\n",
    "# 2\n",
    "### zero\n",
    "df = train_x_2_zero\n",
    "scale = scaler.fit_transform(df)\n",
    "train_x_2_zero = pd.DataFrame(data=scale, columns=df.columns, index=df.index)\n",
    "\n",
    "df = test_x_2_zero\n",
    "scale = scaler.transform(df)\n",
    "test_x_2_zero = pd.DataFrame(data=scale, columns=df.columns, index=df.index)\n",
    "\n",
    "\n",
    "### drop\n",
    "df = train_x_2_drop\n",
    "scale = scaler.fit_transform(df)\n",
    "train_x_2_drop = pd.DataFrame(data=scale, columns=df.columns, index=df.index)\n",
    "\n",
    "df = test_x_2_drop\n",
    "scale = scaler.transform(df)\n",
    "test_x_2_drop = pd.DataFrame(data=scale, columns=df.columns, index=df.index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4)_ train / valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# zero\n",
    "z_train_x_1, z_valid_x_1, z_train_y_1, z_valid_y_1= train_test_split(train_x_1_zero, train_y_1, test_size=0.15)\n",
    "z_train_x_2, z_valid_x_2, z_train_y_2, z_valid_y_2= train_test_split(train_x_2_zero, train_y_2, test_size=0.15)\n",
    "\n",
    "# drop\n",
    "d_train_x_1, d_valid_x_1, d_train_y_1, d_valid_y_1= train_test_split(train_x_2_drop, train_y_2, test_size=0.15)\n",
    "d_train_x_2, d_valid_x_2, d_train_y_2, d_valid_y_2= train_test_split(train_x_1_drop, train_y_1, test_size=0.15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Build model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1)_ define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "# metric\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "xgbc = XGBClassifier(eval_metric='mlogloss')\n",
    "svmc = svm.SVC()\n",
    "gbc = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "models = [rfc, xgbc, svmc, gbc]\n",
    "model_names = ['RandomForestClassifier','XGBClassifier','SVC','GradientBoostingClassifier']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2)_ fit, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### model : 1 ###\n",
      "\n",
      "✅ zero\n",
      "\n",
      "RandomForestClassifier : 0.6842105263157895\n",
      "XGBClassifier : 0.5789473684210527\n",
      "SVC : 0.39473684210526316\n",
      "GradientBoostingClassifier : 0.6842105263157895\n",
      "\n",
      "✅ drop\n",
      "\n",
      "RandomForestClassifier : 0.8679245283018868\n",
      "XGBClassifier : 0.8679245283018868\n",
      "SVC : 0.8490566037735849\n",
      "GradientBoostingClassifier : 0.8301886792452831\n",
      "\n",
      "\n",
      "\n",
      "### model : 2 ###\n",
      "\n",
      "✅ zero\n",
      "\n",
      "RandomForestClassifier : 0.8301886792452831\n",
      "XGBClassifier : 0.8301886792452831\n",
      "SVC : 0.8301886792452831\n",
      "GradientBoostingClassifier : 0.8490566037735849\n",
      "\n",
      "✅ drop\n",
      "\n",
      "RandomForestClassifier : 0.5789473684210527\n",
      "XGBClassifier : 0.5263157894736842\n",
      "SVC : 0.3684210526315789\n",
      "GradientBoostingClassifier : 0.5263157894736842\n"
     ]
    }
   ],
   "source": [
    "print('### model : 1 ###\\n')\n",
    "\n",
    "print('✅ zero\\n')\n",
    "for idx, i in enumerate(models):\n",
    "    warnings.filterwarnings('ignore')\n",
    "    model = i\n",
    "    model.fit(z_train_x_1, z_train_y_1)\n",
    "    pred = model.predict(z_valid_x_1)\n",
    "    print('{} :'.format(model_names[idx]), accuracy_score(pred, z_valid_y_1))\n",
    "\n",
    "print('\\n✅ drop\\n')\n",
    "for idx, i in enumerate(models):\n",
    "    warnings.filterwarnings('ignore')\n",
    "    model = i\n",
    "    model.fit(d_train_x_1, d_train_y_1)\n",
    "    pred = model.predict(d_valid_x_1)\n",
    "    print('{} :'.format(model_names[idx]), accuracy_score(pred, d_valid_y_1))\n",
    "\n",
    "\n",
    "print('\\n\\n\\n### model : 2 ###\\n')\n",
    "\n",
    "print('✅ zero\\n')\n",
    "for idx, i in enumerate(models):\n",
    "    warnings.filterwarnings('ignore')\n",
    "    model = i\n",
    "    model.fit(z_train_x_2, z_train_y_2)\n",
    "    pred = model.predict(z_valid_x_2)\n",
    "    print('{} :'.format(model_names[idx]), accuracy_score(pred, z_valid_y_2))\n",
    "\n",
    "print('\\n✅ drop\\n')\n",
    "for idx, i in enumerate(models):\n",
    "    warnings.filterwarnings('ignore')\n",
    "    model = i\n",
    "    model.fit(d_train_x_2, d_train_y_2)\n",
    "    pred = model.predict(d_valid_x_2)\n",
    "    print('{} :'.format(model_names[idx]), accuracy_score(pred, d_valid_y_2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3)_ feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8301886792452831\n"
     ]
    }
   ],
   "source": [
    "model = rfc\n",
    "\n",
    "# 1\n",
    "model.fit(d_train_x_1, d_train_y_1)\n",
    "pred1 = model.predict(d_valid_x_1)\n",
    "print(accuracy_score(pred1, d_valid_y_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8867924528301887\n"
     ]
    }
   ],
   "source": [
    "model = gbc\n",
    "\n",
    "# 2\n",
    "model.fit(z_train_x_2, z_train_y_2)\n",
    "pred2 = model.predict(z_valid_x_2)\n",
    "print(accuracy_score(pred2, z_valid_y_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m importance_values \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfeature_importances_\n\u001b[1;32m      6\u001b[0m \u001b[39m# 정렬과 시각화를 쉽게 하기 위해 series 전환\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m ft_series \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries(importance_values, index \u001b[39m=\u001b[39m d_train_x_1\u001b[39m.\u001b[39;49mcolumns)\n\u001b[1;32m      8\u001b[0m ft_top20 \u001b[39m=\u001b[39m ft_series\u001b[39m.\u001b[39msort_values(ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)[:\u001b[39m100\u001b[39m]\n\u001b[1;32m     10\u001b[0m \u001b[39m# 시각화\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "importance_values = model.feature_importances_\n",
    "\n",
    "# 정렬과 시각화를 쉽게 하기 위해 series 전환\n",
    "ft_series = pd.Series(importance_values, index = d_train_x_1.columns)\n",
    "ft_top20 = ft_series.sort_values(ascending=False)[:100]\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title('Feature Importance Top 2850')\n",
    "sns.barplot(x=ft_top20, y=ft_top20.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "model.fit(z_train_x_2, z_train_y_2)\n",
    "pred2 = model.predict(z_valid_x_2)\n",
    "print(accuracy_score(pred2, z_valid_y_2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ zero\n",
    "\n",
    "### 1\n",
    "train_x_1_zero\n",
    "test_x_1_zero\n",
    "### 2\n",
    "train_x_2_zero\n",
    "test_x_2_zero\n",
    "\n",
    "# ✅ drop\n",
    "\n",
    "### 1\n",
    "train_x_1_drop\n",
    "test_x_1_drop\n",
    "### 2\n",
    "train_x_2_drop\n",
    "test_x_2_drop\n",
    "\n",
    "train_x1 = train_x_1_drop\n",
    "test_x1 = test_x_1_drop\n",
    "\n",
    "train_x2 = train_x_2_zero\n",
    "test_x2 = test_x_2_zero\n",
    "\n",
    "# rfc, xgbc, svmc, gbc\n",
    "# 'RandomForestClassifier','XGBClassifier','SVC','GradientBoostingClassifier'\n",
    "\n",
    "\n",
    "\n",
    "# 1\n",
    "model = rfc\n",
    "model.fit(train_x_1_drop, train_y_1)\n",
    "pred1 = model.predict(test_x_1_drop)\n",
    "\n",
    "# 2\n",
    "model = gbc\n",
    "model.fit(train_x2, train_y_2)\n",
    "pred2 = model.predict(test_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_pred_value = np.concatenate([pred1, pred2])\n",
    "\n",
    "submit_csv = pd.read_csv('./sample_submission.csv')\n",
    "submit_csv['Y_Class'] =sub_pred_value\n",
    "submit_csv.to_csv('0208_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9acc76a459d7db86261bc3c0fc1f5dcdadf57ce90dda3656ef3156477f1c2288"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
